---
layout: post
title: Where Do I Go From Here?
---

With my 12 weeks of bootcamp behind me, the question now is, Where do I go from here? Next Monday I will not start the morning working with a partner on a coding problem. I won't have interactive lectures, and I won't be able to schedule one-on-one time with knowledgable instructors. I won't be set completely adrift, thanks to the fantastic network of fellow data scientists I've been building through Metis, as well as the curriculum and all my accumulated work I have to reference. But for the most part, I'm on my own starting from now. As a way of staking out my next steps towards finding a job and eventually greater success in data science, I'm writing this blog on the future work I plan on doing.

### Improve Existing Metis Projects

During my time at Metis, I created four independent projects covering a wide range of data science techniques. There are definitely improvements to be made to each, but I plan on focusing on my last two, largely because these were the topics that interested me the most.

## Jeopardy! Categories

My fourth project at Metis used NLP techniques to categorize questions and answers into more a more managable number of categories. The [data was found on Kaggle](https://www.kaggle.com/tunguz/200000-jeopardy-questions), where someone had scraped together a database of over 200,000 questions from [J! Archive](https://j-archive.com/). Each row of the data was a different question, along with the answer, category, point value, air date, round, and show number. Of course, questions on Jeopardy! already *have* categories, but they aren't categorized in any way that makes them usefully sorted for, say, someone practicing by going through all the old questions. I thought it would be useful to group the questions into common trivia categories, like sports or history or film. So, by performing NMF topic modeling with a TF-IDF vectorizer, I came up with 25 categories to bin each question into. In addition, I made a rudimentary application which allows players to cycle through random questions until getting one wrong, at which point you can serve up a question considered "most similar" to the one you'd gotten based on cosine distance of the topic vectors.

I was actually unaware when I first scoped out the project that one of my favorite Jeopardy! champions - Roger Craig, who happened to be getting his PhD in Computer Science at the time - had [done a similar, though more advanced, project of his own](https://vimeo.com/29001512) which helped him to get on the show and set what was at the time a single-day record of $77,000. As soon as I found this out, I wanted to incorporate some of his more advanced features into my project, but I did not have nearly enough time while I was busy digesting all that NLP material for the first time to do that. My future plans for this project are to get those features added in.

The primary advantages of Roger's project were the inclusion of the monetary value of the questions, the frequency of certain categories, and tracking a player's performance over time. As such, his model could estimate the most "valuable" questions to focus on (i.e. questions from categories that are more valuable on average, that appear more often than average, and that the player is worse at answering on average), as well as allow him to see where he was making the biggest gains. I plan to work on making these improvements to my project, and I would also like to make a more sophisticated application, ideally one which could be deployed publicly.

## Bird Image Classification

My [last blog post](https://willstith.github.io/2020/09/17/Bird-Up/) detailed my final project at Metis, which was a classifier for bird images. With it, I am able to sort my bird photos into specific folders corresponding to the bird identified in each image...with 61.9% accuracy across 36 species. Pretty good! But clearly it could do better. Previous groups have reported >90% accuracy across 200 species, so it's undeniably possible. These groups used more sophisticated convolutional neural network architectures, as well as augmentation of training images in some cases. My primary goal to improve this project is to implement Mask R-CNN, a relatively new model architecture which outperforms anything I incorporated. Mask R-CNN is able to detect objects in images and create a pixel-wise mask over that object, which tightly narrows the focus of later layers of a neural net. I spent much of the time working on this project trying to implement it, but alas, I am a noob, and it isn't so easy.

I'd also like to add more of my own bird photos and include more classes once I have sufficient number of photos of new species. The more training examples, the better! Of course, as my training data gets large enough, I'll need to think about ways to speed up the model (it already takes several seconds to sort each image...).

Lastly, I'd like to add an additional step to the program where photos with no bird at all are sorted into a separate folder. This may require the use of Mask R-CNN, but I think there are other, perhaps less effective, ways I could do it. This was another thing I tried to get working before the project due date, but was unable. My initial idea was to use predictions of another model pre-trained on a "birds" class as a way of filtering out the images that don't contain one. But ImageNet appears to only have certain specific species labelled, and even by grouping those together, the accuracy wasn't impressive. Not even worth incorporating for the time being.

### Future Project Ideas

The
